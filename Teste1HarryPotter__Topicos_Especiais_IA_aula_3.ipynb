{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jcmacuacua/Dados_Hyperspetral/blob/main/Teste1HarryPotter__Topicos_Especiais_IA_aula_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Ekyb0qhDVuCj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open ('/content/drive/MyDrive/Text/Harry Potter e o Enigma do Prin - J.K. Rowling.txt') as f:\n",
        "  livroHarry = f.read()\n",
        "print('Harry Potter:', len(livroHarry))\n",
        "print (livroHarry[1:100])"
      ],
      "metadata": {
        "id": "lZTGCgawuHE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "yomlK5Vqvn-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize"
      ],
      "metadata": {
        "id": "N5Lg_pCWwpxV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "livroHarry = re.sub(r'\\n', '', livroHarry)\n",
        "#sentencasEx = sent_tokeniza(exem1, language='portuguese')\n",
        "sentencasHarry = sent_tokenize(livroHarry, language='portuguese')"
      ],
      "metadata": {
        "id": "5QDSyzaLw4MK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "# remove as quebras de linhas do arquivo texto\n",
        "livroHarry1 = re.sub(r'\\n', ' ', livroHarry)\n",
        "#sentencasEx = sent_tokenize(exem1, language='portuguese')\n",
        "sentencasHarry = sent_tokenize(livroHarry1, language='portuguese')\n",
        "\n",
        "def imprimeSentencas(sentencas, qde):\n",
        "  limTeste = min (len(sentencas), 10)\n",
        "  for s in range(0, limTeste):\n",
        "      print(sentencas[s])\n",
        "\n",
        "#print('Exemplo. Num de sentencas: ', len(sentencasEx))\n",
        "#imprimeSentencas(sentencasEx,10)\n",
        "print('\\nHarry Potter, Num de sentencas: ',len(sentencasHarry))\n",
        "imprimeSentencas(sentencasHarry,10)"
      ],
      "metadata": {
        "id": "84-Ovtw_xWG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "\n",
        "swPort = nltk.corpus.stopwords.words('portuguese')\n",
        "print('Stop-words: ', swPort )\n",
        "\n",
        "def removeSimbolos(listaSentencas):\n",
        "    return [ re.sub(r'[^\\w]',' ', s) for s in listaSentencas ]\n",
        "\n",
        "sentencasHarry2 = removeSimbolos(sentencasHarry)\n",
        "for i in range(10):\n",
        "    print( sentencasHarry[i] )\n",
        "    print( sentencasHarry2[i] )\n",
        "\n",
        "#sentencasEx2 = removeSimbolos(sentencasEX)"
      ],
      "metadata": {
        "id": "vmzeDXiL0zkL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# remove os stop-words\n",
        "def removeStopwords(listaSentencas, listaSW):\n",
        "    resultado = list(listaSentencas)\n",
        "    for i in range(len(listaSentencas)):\n",
        "        s = re.sub(r'\\s+', ' ', listaSentencas[i])\n",
        "        s2 = nltk.word_tokenize(s, language='portuguese')\n",
        "        s3 = [w.lower() for w in s2 if w.lower() not in listaSW]\n",
        "        resultado[i] = s3\n",
        "    return resultado\n",
        "#sentencasEx3 = removeStopwords(sentencasEx2,swPort)\n",
        "sentencasHarry3 = removeStopwords(sentencasHarry2,swPort)\n",
        "\n",
        "print('\\n\\nApos remover as stop-words')\n",
        "for i in range(10):\n",
        "    print( sentencasHarry2[i])\n",
        "    print( sentencasHarry3[i])\n"
      ],
      "metadata": {
        "id": "EAP2-kGU3A2u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing import text\n",
        "from keras.utils import np_utils\n",
        "from keras.preprocessing import sequence"
      ],
      "metadata": {
        "id": "5jmpHZqtiNbq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#vejam que estou usando o texto original aqui\n",
        "def rodaToken(original):\n",
        "    tokenizer = text.Tokenizer()\n",
        "    words = text.text_to_word_sequence( original,\n",
        "        filters= '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
        "        lower=True,\n",
        "        split=' ' )\n",
        "    tokenizer.fit_on_texts(words)\n",
        "    word2id = tokenizer.word_index\n",
        "    vocab_size = len(word2id)\n",
        "    print('Vocabulary Size:', vocab_size)\n",
        "    print('Vocabulary Sample:', list(word2id.items())[:20])\n",
        "    print( words[:15])\n",
        "\n",
        "#print('Exemplo inicial')\n",
        "#rodaToken(exem1)\n",
        "print('Harry Potter')\n",
        "rodaToken(livroHarry)"
      ],
      "metadata": {
        "id": "E2QBmgyEiR8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# analise sem as stop-words\n",
        "def criaVocabulario(listaSentencas):\n",
        "    oneDim = [w for s in listaSentencas for w in s ]\n",
        "    #conjunto = set(listaSentencas)\n",
        "    return sorted(list(set(oneDim)))\n",
        "\n",
        "wordsHar3 = criaVocabulario(sentencasHarry3)\n",
        "print('Pequeno principe - words: ',len(wordsHar3))\n",
        "print( wordsHar3[:20] )"
      ],
      "metadata": {
        "id": "BAYfIjkoj9d2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cria os vetores \n",
        "def criaVetores(sentencas, words):\n",
        "    res = list(sentencas)\n",
        "    for i in range(0,len(sentencas)):\n",
        "        s = sentencas[i]\n",
        "        nova = []\n",
        "        for w in s:\n",
        "            if w in words:\n",
        "                nova.append( words.index(w) )\n",
        "        res[i] = nova\n",
        "    return res\n",
        "\n",
        "vetHar = criaVetores(sentencasHarry3,wordsHar3)\n",
        "print(sentencasHarry3)\n",
        "print(wordsHar3)\n",
        "print(vetHar)"
      ],
      "metadata": {
        "id": "FqQV3MGwm_D5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "uMSqCd7ctQTP"
      }
    }
  ]
}